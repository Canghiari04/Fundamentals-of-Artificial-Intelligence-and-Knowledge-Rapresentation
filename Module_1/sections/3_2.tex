\textbf{A* search} is the most known case of best-first search algorithm\footnote{Basically, A* search try to combine the benefits of DFS, efficiency, with the ones of
uniform-cost search, optimality and completeness.}. It evaluates nodes for the expansion combining $g(n)$, the cost to reach the node $n$, and 
$h(n)$, the cost to get from the node $n$ to the goal state. \vspace{3.5pt}

\begin{center}
    $f(n) = g(n) + h(n)$
\end{center} \vspace{3.5pt}

In this case, the evaluation function $f(n)$ give us the estimated cost of the cheapest solution through $n$. The algorithm is very similar to uniform-cost search, but instead
of using only $g(n)$, it associates each node with the sum $g(n) + h(n)$. The next node selected for the expansion will be the state with the lowest value of $g(n) + h(n)$. \vspace{3.5pt}

Provided the heuristic function $h(n)$ that satisfies certain conditions, A* search is both \textbf{complete} and \textbf{optimal}; if the heuristic function does not meet 
certain criteria, A* search does not guarantee to find the optimal solution. Which are these conditions? \vspace{3.5pt}

\textbf{Feasibility} \vspace{3.5pt}

The first condition required for optimality is that the heuristic function $h(n)$ is a \textbf{feasible} heuristic. A feasible heuristic is one that \textit{never 
overstimates} the real cost to reach the goal state. \vspace{3.5pt}

We suppose that with $h^*(n)$ we indicate the true distance between the current node and the goal state. The heuristic function $h(n)$ is feasible if we always have, for any
node $n$ of the search tree, that: \vspace{3.5pt}

\begin{center}
    $h(n) \le h^*(n)$.
\end{center} \vspace{3.5pt}

Ensured this condition, the heuristic $h(n)$ is feasible.
\begin{definition}[title={Optimality theorem}]
    If $h(n) \le h^*(n)$ for each node, then the A* search algorithm always finds the optimal path to the goal state.
\end{definition}

Let's see an example for better understanding.
\begin{example}
    i.e. Optimality proof. \vspace{3.5pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{img/img5.png}
    \end{center} \vspace{3.5pt}

    Given this search tree, we have:
    \begin{itemize}
        \renewcommand{\labelitemi}{-}
        \item \textbf{G}: the goal node or the goal state.
        \item \textbf{Start}: the state representing the root node.
        \item \textbf{n}: which is the unexpanded node already present in the priority queue, such that \textbf{n} is on the optimal path.
        \item \textbf{$\mathbf{G_2}$}: which defines the \textbf{sub-optimal} path. In this data structure, we are reaching the same goal node \textbf{G} from different trails.
    \end{itemize} \vspace{7pt}

    We need to show that, if the A* algorithm has inside its queue a sub-optimal goal $\mathbf{G_2}$ and a node $\mathbf{n}$ which is on the optimal path to the goal node 
    $\mathbf{G}$, the evaluation function $f(n)$ will be less or equal to the evaluation function $f(G_2)$, denyng to A* algorithm to expand the node $\mathbf{G_2}$ first. \vspace{3.5pt}

    \textbf{1. Evaluation of the optimal goal.} 
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        The evaluation function of \textbf{G} is: \vspace{3.5pt}
        \begin{center}
            $f(G) = g(G) + h(G)$
        \end{center} \vspace{3.5pt}

        $h(G)$ is equal to $0$, because the cost to reach the goal node itself cannot be greater than $0$. \vspace{3.5pt}
        \begin{center}
            $f(G) = g(G) + h(G) = g(G) + 0 = g(G)$ \\
            $f(G) = g(G)$.
        \end{center}
    \end{itemize}

    \textbf{2. Evaluation of the sub-optimal goal.} 
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        The evaluation function of $\mathbf{G_2}$ is: \vspace{3.5pt}
        \begin{center}
            $f(G_2) = g(G_2) + h(G_2)$
        \end{center} \vspace{3.5pt}

        from the same reasons described in the first point, the evaluation function is equal to the real-cost function: \vspace{3.5pt}
        \begin{center}
            $f(G_2) = g(G_2)$.
        \end{center}
    \end{itemize}

    \textbf{3. Comparison of G and $\mathbf{G_2}$.} 
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        Knowing that $\mathbf{G_2}$ is the sub-optimal path, the total cost to reach the goal node via $\mathbf{G_2}$ must be strictly greater than the total cost to reach the
        goal node via $\textbf{G}$, so: \vspace{3.5pt}
        \begin{center}
            $g(G_2) > g(G)$
        \end{center} \vspace{3.5pt}
        
        the same condition can be expressed as follows:  \vspace{3.5pt}
        \begin{center}
            $f(G_2) > f(G)$.
        \end{center} \vspace{3.5pt}
    \end{itemize}

    \textbf{3. Evaluation of node n.} 
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        Given \textbf{n} a node not expanded, but already present in the priority queue, the evaluation function $f(n)$ is: \vspace{3.5pt}
        \begin{center}
            $f(n) = g(n) + h(n)$
        \end{center} \vspace{3.5pt}

        from the \textbf{optimality theorem}, the estimated cost of \textbf{n} must be less or equal to the real cost of \textbf{n}: \vspace{3.5pt}
        \begin{center}
            $g(n) + h(n) \le g(n) + h^*(n)$
        \end{center} \vspace{3.5pt}

        where the function $h^*(n)$ defines the real cost to reach the goal from \textbf{n}. Therefore, knowing that \textbf{n} is on the optimal path, the sum of the cost 
        to reach \textbf{n} and the real-cost between \textbf{n} and \textbf{G}, is exactly the same effort to achieve \textbf{G}. \vspace{3.5pt}
        \begin{center}
            $f(n) = g(n) + h^*(n) = g(G)$
        \end{center} \vspace{3.5pt}

        Replacing $g(G)$ with $f(G)$, from the first point, we have: \vspace{3.5pt}
        \begin{center}
            $f(G) = g(n) + h^*(n)$
        \end{center} \vspace{3.5pt}

        finally: \vspace{3.5pt}
        \begin{center}
            $f(n) \le g(n) + h^*(n) = f(G)$ \\
            $f(n) \le f(G)$.
        \end{center} \vspace{3.5pt}
    \end{itemize}

    \textbf{4. Conclusion.} 
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        Combining the results:\vspace{3.5pt}
        \begin{center}
            $f(n) \le f(G)$ and $f(G) < f(G_2)$
        \end{center} \vspace{3.5pt}

        the final conclusion is:\vspace{3.5pt}
        \begin{center}
            $f(n) < f(G_2)$.
        \end{center} 
    \end{itemize} \vspace{7pt}

    We can conclude saying that the heuristic function $h(n)$ is \textbf{feasible}, the A* algorithm will define always \textbf{optimal} solutions for search trees.  
\end{example} \vspace{7pt}

\textbf{Consistency} \vspace{3.5pt}

The second condition is slightly stronger than feasibility, and it is required only when A* algorithm is applied to search graph\footnote{Until now, we have assumed so far that the search space is a tree not a graph.}.
The A* algorithm for search graphs is optimal and complete if the heuristic function $h(n)$ is \textbf{consistent}. A heuristic function $h(n)$ is consistent if, for every node
\textbf{n} and every successor $\mathbf{n^*}$ of \textbf{n} generated by an action \textbf{a}, the estimated cost of reaching the goal node \textbf{G} from \textbf{n} is no 
greater than the step cost of getting $\mathbf{n^*}$ and reach the goal \textbf{G} from it: \vspace{3.5pt}
\begin{center}
    $h(n) \le c(n, a, n^*) + h(n^*)$.
\end{center} \vspace{3.5pt}

This is a form of the \textbf{triangle inequality}, which says that each side of the triangle cannot be longer than the sum of the other two sides. The inequality makes 
perfect sense either for feasibility condition: if there were a trail from \textbf{n} to \textbf{G} via $\mathbf{n^*}$ cheaper than $h(n)$, that would violate the 
\textbf{feasibility} condition\footnote{We remember that for feasibility condition we mean: for every node $n$, the heuristic function $h(n)$ is always less or equal to the true distance between the node $n$ and the goal node $G$ ($h(n) \le h^*(n)$).}. 
\begin{definition}[title={Consistency theorem}]
    If $h(n)$ is consistent then A* search applied to graphs is optimal.
\end{definition}
\begin{example}
    i.e. Consistency proof. \vspace{3.5pt}
    \begin{center}
        \includegraphics[width=0.25\textwidth]{img/img6.png}
    \end{center} \vspace{3.5pt}

    We want to prove that, if $h(n)$ is consistent, then the values of $f(n)$ along any path are non-descresing. \vspace{7pt}

    \textbf{1. Defining successor $\mathbf{n^*}$ of $\mathbf{n}$.}
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        Suppose $\mathbf{n^*}$ is a successor of the node $\mathbf{n}$, then the evaluation function is defined as: \vspace{3.5pt}
        \begin{center}
            $f(n^*) = g(n^*) + h(n^*)$
        \end{center} \vspace{3.5pt}

        if $\mathbf{n^*}$ is a successor of \textbf{n}, the real-cost function can be expressed as: \vspace{3.5pt}
        \begin{center}
            $g(n^*) = g(n) + c(n, a, n^*)$
        \end{center} \vspace{3.5pt}

        therefore, the evaluation function $f(n^*)$ becomes: \vspace{3.5pt}
        \begin{center}
            $f(n^*) = g(n) + c(n, a, n^*) + h(n^*)$.
        \end{center} \vspace{3.5pt}
    \end{itemize}

    \textbf{2. Comparison of $\mathbf{f(n^*)}$ and f(n)}
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        As we know, the evaluation function of \textbf{n} is: \vspace{3.5pt}
        \begin{center}
            $f(n) = g(n) + h(n)$
        \end{center} \vspace{3.5pt}

        and from the previous step the evaluation function of $\mathbf{n^*}$ is: \vspace{3.5pt}
        \begin{center}
            $f(n^*) = g(n) + c(n, a, n^*) + h(n^*)$
        \end{center} \vspace{3.5pt}

        combining the two results we have: \vspace{3.5pt}
        \begin{center}
            $g(n) + c(n, a, n^*) + h(n^*) \ge g(n) + h(n)$.
        \end{center} \vspace{3.5pt}
    \end{itemize}

    \textbf{3. Conclusion}
    \begin{itemize}
        \renewcommand{\labelitemi}{}
        \item \vspace{3.5pt}
        Knowing that: \vspace{3.5pt}
        \begin{center}
            $f(n) = g(n) + h(n)$ \\
            $f(n^*) = g(n) + c(n, a, n^*) + h(n^*)$
        \end{center} \vspace{3.5pt}

        we can substitute the terms of the previous inequality, that becomes: \vspace{3.5pt}
        \begin{center}
            $f(n) \le f(n^*)$.
        \end{center} \vspace{3.5pt}
    \end{itemize}

    Since the evaluation function $f(n^*)$ never decreases, we can conclude saying that the heuristic function $h(n)$ is \textbf{consistent}, the A* algorithm will define 
    \textbf{optimal} solutions for search graphs.
\end{example}