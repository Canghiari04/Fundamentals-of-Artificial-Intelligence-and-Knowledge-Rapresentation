Every agent based on \textbf{decision theory} needs a formal language to use and represent probabilistic informations. Typically AI needs a more suited and consistent approach than the traditional probability theory. This section includes all the necessary definitions and examples to understand the subsequent arguments in depth.
\begin{definition}
    The set of all possible worlds is called the \textbf{sample space}, denoted $\Omega$. Any subset $A\subseteq\Omega$ is an \textbf{event}. Any element $\omega\in\Omega$ is called \textbf{sample point}.    
\end{definition}
\begin{definition}
    A \textbf{probability space} is a sample space with an assignment $P(\omega)$ for every $\omega\in\Omega$ where:
    \begin{itemize}
        \renewcommand{\labelitemi}{-}
        \item $0 \leq P(\omega) \leq 1$
        \item $\sum P(w) = 1$ for every $\omega\in\Omega$
    \end{itemize}
\end{definition}
\begin{definition}
    A \textbf{random variable} is a function from sample points to some range, e.g., the reals or Booleans. \vspace{3.5pt}

    e.g. \textit{Odd(1) = true}
\end{definition}
\begin{definition}
    \textit{\textbf{P}} induces a \textbf{probability distribution} for any random variable \textit{\textbf{X}}: \vspace{3.5pt}
    
    \begin{center}
        $P(X=x_i) = \sum_{\omega:X(\omega)=x_i} P(\omega)$
    \end{center} \vspace{3.5pt}
    
    A \textbf{probability distribution} gives values for all possible assignment.
\end{definition}
\begin{definition}
    \textbf{Prior} or \textbf{unconditional probabilities} of propositions correspond to belief prior to arrival of any new evidence. \vspace{3.5pt}

    e.g. \textit{P(Cavity = True) = 0.1}
\end{definition}
\begin{definition}
    The \textbf{Joint Probability Distribution} for a set of random variables gives the probability of every sample point on those random variables. \vspace{3.5pt}

    e.g. \textit{\textbf{P}(Weather, Cavity) = a $2\times4$ matrix of values:}
    \begin{center}
        \begin{table}[H]
            \centering
            \begin{tabular}{|l|c|c|c|c|}
                \hline
                \it Weather = & \it sunny & \it rain & \it cloudy & \it snow \\
                \cline{1-5}
                \it Cavity = True & 0.144 & 0.02 & 0.016 & 0.02 \\
                \it Cavity = False & 0.576 & 0.08 & 0.064 & 0.08 \\
                \hline
            \end{tabular}
            \caption{Probability distribution of the Weather random variable}
            \label{t_1_1}
        \end{table}
    \end{center}
    Every question about a certain domain can be answered by the joint distribution because every event is a sum of sample points.
\end{definition}
\begin{definition}
    A function $p:R \rightarrow R$ is a \textbf{probability density function} (\textbf{pdf}) for \textit{X} if it is a nonnegative integrable function s.t. \vspace{3.5pt}

    \begin{center}
        $\int_{Val(X)} p(x)dx = 1$
    \end{center}
\end{definition}
\begin{definition}
    \textbf{Conditional} or \textbf{posterior probabilities} $P(X|Evidence)$ represent a more informed distribution in the light of new \textbf{evidence}. \vspace{3.5pt}

    e.g. $P(cavity|toothache) = 0.8$ \vspace{3.5pt}

    It does not mean \textit{"if I have toothache then there is $80\%$ of chance that there is also a cavity"}, instead the evidence mean \textit{"given toothache evidence is all I know"}. \vspace{3.5pt}

    The typically definition of conditional or posterior probability is: \vspace{3.5pt}

    \begin{center}
        $P(a|b) = \frac{P(a \land b)}{P(b)}$ if $P(b)\neq0$
    \end{center} \vspace{3.5pt}

    Otherwise, numerator can be written by the \textbf{product rule}: \vspace{3.5pt}
    
    \begin{center}
        $P(a \land b) = P(a|b)P(b) = P(b|a)P(a)$
    \end{center} \vspace{3.5pt}

    The product rule at the same time is applied to whole distributions, not only for single values as done previously. \vspace{3.5pt}
    
    \begin{center}
        $\mathbf{P}(Weather, Cavity) = \mathbf{P}(Weather|Cavity)\mathbf{P}(Cavity)$
    \end{center} \vspace{3.5pt}
\end{definition}