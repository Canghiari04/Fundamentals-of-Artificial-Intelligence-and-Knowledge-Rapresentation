\textbf{Rejection sampling} is a general method for computing conditional probabilities. The algorithm, shown below, is very simple, it generates samples from the prior 
distribution specified by the network, and it discards all those samples that do not match with the given evidences. Finally, the estimated probability $\hat{\mathbf{P}}(X|\mathbf{e})$
is obtained by counting how often $X = x$ occurs in the remaining set of samples.
\begin{example}
    i.e. Continuing with our example from \textbf{Sampling method for empty network} one. \vspace{3.5pt}

    Let us assume the probability distribution of \vspace{3.5pt}
    \begin{center}
        $\mathbf{P}(Rain|Sprinkler=True) = ?$,
    \end{center} \vspace{3.5pt}
    using $100$ samples. Of the 100 generated, suppose that $73$ have $Sprinkler=False$ and are discarded, while $27$ have $Sprinkler=True$; of the $27$, $8$ have $Rain=True$ 
    and the last $19$ have $Rain=False$. Hence, \vspace{3.5pt}
    \begin{center}
        $\mathbf{P}(Rain|Sprinkler=True) = \alpha(\langle8, 19\rangle) = \langle0.296, 0.704\rangle$.
    \end{center} \vspace{3.5pt}
    The true answer is $\langle0.3, 0.7\rangle$. As more samples are collected, the estimate will converge to the true probability. 
\end{example}
Before moving on, let $\hat{\mathbf{P}}(X|\mathbf{e})$ be the estimated probability that the algorithm returns. From the definition of the algorithm, we have: \vspace{3.5pt}
\begin{center}
    $\hat{\mathbf{P}}(X|\mathbf{e}) = \alpha\mathbf{N}_{PS}(X, \mathbf{e}) = \frac{\mathbf{N}_{PS}(X, \mathbf{e})}{N_{PS}(\mathbf{e})}$.
\end{center} \vspace{3.5pt}
From the previous equation\footnote{$P(x_1 \dots x_n) \approx \frac{N_{PS}(x_1, \dots, x_m)}{N}$.}, this becomes:
\begin{center}
    $\hat{\mathbf{P}}(X|\mathbf{e}) \approx \frac{\mathbf{P}(X, \mathbf{e})}{P(\mathbf{e})} = \mathbf{P}(X|\mathbf{e})$.
\end{center}
Rejection sampling produces \textbf{consistent} estimate of the true probability\footnote{Note that we are not comparing the estimated distribution with the prior joint distribution, but instead with the posterior distribution. Here, we are talking about conditional probabilities, not anymore of samples assigned to each random variable that composed the network.}.

However, rejection sampling is affected by a major problem. Previously, we said that the estimated probability will converge to the true probability as the number of samples
collected grows. But, as the number of evidence variables grows the fraction of samples ($N_{PS}(\mathbf{e})$) drops exponentially! Therefore, rejection sampling is not 
a good choice when the Bayesian network seems to be too complex.