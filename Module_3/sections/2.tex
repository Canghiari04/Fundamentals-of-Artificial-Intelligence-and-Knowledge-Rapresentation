If we expand the full joint distribution defined in Figure \ref{t_1_2} by adding a new random variable, \textit{Weather}, it becomes $\mathbf{P}(Weather, Toothache, Cavity, Catch)$, which has $2\times2\times2\times4$ = $32$ entries.
But, what is the relationship between these four random variables? For instance, are the $P(cloudy, toothache, cavity, catch)$ and $P(toothache, cavity, catch)$ related? This last question can be expressed in probabilistic terms as:
\begin{center}
    $P(cloudy, toothache, catch, cavity) = P(cloudy|tootchache, cavity, catch)P(toothache, cavity, catch)$
\end{center}
At the same time, we can imagine that \textit{Toothache, Cavity, Catch} should be independent from \textit{Weather}. Therefore, the following assertion seems reasonable:
\begin{center}
    $P(cloudy|toothache, catch, cavity) = P(cloudy)$
\end{center}
From this, we can deduce:
\begin{center}
    $P(cloudy, toothache, catch, cavity) = P(cloudy)P(toothache, catch, cavity)$
\end{center}
Or generally:
\begin{center}
    $\mathbf{P}(Weather, Toothache, Catch, Cavity) = \mathbf{P}(Toothache, Catch, Cavity)\mathbf{P}(Weather)$
\end{center}
Thus, the initial $32$ entries table can be divided from one $8$-entries table and one $4$-entries table. The property used in the previously equation is called \textbf{independence}.

First of all are introduced some basic definitions and examples to understand the effectiveness of independence.
\begin{definition}
    \textit{A} and \textit{B} are \textbf{independent}, denoted $\mathbf{P} \models (A \perp B)$, if and only if \\
    $\mathbf{P}(A|B) = \mathbf{P}(A)$ or $\mathbf{P}(B|A) = \mathbf{P}(B)$ or $\mathbf{P}(A|B) = \mathbf{P}(A)\mathbf{P}(B)$
\end{definition}
When they are available, independence assertions can help in reducing the size of the domain representation and the complexity of the inference problem. Unfortunately, clean separation of entire sets of variables by independence are quite rare. Moreover, even the independence subset can be quite large, for instance, dentistry might involve dozens of diseases and symptoms, all of which are associated. To handle such problems, we need more specific methods than the general concept of independence, one of them is named \textbf{conditional independence}. Let see an example of conditional independence.
\begin{example}
    i.e. given $\mathbf{P}(Toothache, Cavity, Catch)$ has $2^3 - 1 = 7$ independent entries \ownfootnote{\it Why 7 independent entries and not 8 as before? Simply, if we know 7 of them the 8th is automatically determined, must be the last value remaining.}. If I have a cavity, the probability that the probe catches in it does not depend on whether I have toothache: \vspace{3.5pt}
    \begin{center}
        $P(catch|toothache, cavity) = P(catch|cavity)$
    \end{center} \vspace{3.5pt}
    The same independence hold if I haven't got a cavity: \vspace{3.5pt}
    \begin{center}
        $P(catch|toothache, \neg cavity) = P(catch|\neg cavity)$
    \end{center} \vspace{3.5pt}
    Catch is \textbf{conditional indipendent} of Toothache given Cavity \ownfootnote{\it This introduces the meaning of the flow of influence.}. \vspace{3.5pt}
    \begin{center}
        $\mathbf{P}(Catch|Toothache, Cavity) = \mathbf{P}(Catch|Cavity)$ \vspace{3.5pt} \\
        $\mathbf{P} \models (Toothache \perp Catch|Cavity)$
    \end{center} \vspace{3.5pt}
    Using the chain rule, the full joint distribution becomes: \vspace{3.5pt}
    \begin{center}
        $\mathbf{P}(Toothache,Cavity,Catch) =$ \\
        $= \mathbf{P}(Toothache|Catch,Cavity)\mathbf{P}(Catch|Cavity)\mathbf{P}(Cavity)$ \\
        $= \mathbf{P}(Toothache|Cavity)\mathbf{P}(Catch|Cavity)\mathbf{P}(Cavity)$ \\
    \end{center} \vspace{3.5pt}
    $2 + 2 + 1 = 5$ indipendent numbers, we have less entries than before. \vspace{3.5pt}

    In most cases, the use of conditional independence reduces the size of the representation of the joint distribution from \textbf{exponential} to \textbf{linear}.
\end{example}