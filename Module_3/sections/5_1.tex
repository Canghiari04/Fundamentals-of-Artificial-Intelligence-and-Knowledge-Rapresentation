This type of algorithm is the simpliest and works only for those Bayesian networks that have no given evidences. The idea is to sample each random variable that composed the
network, following its topological order. Note that the probability distribution from which the value is sampled is conditionated on the values already assigned to the 
variable's parents. \vspace{3.5pt}

The algorithm, shown below, generates samples from the prior joint distribution specified by the network. Let's consider a new example for better understanding.
\begin{example}
    i.e. Sampling by sprinkler network. \vspace{3.5pt}

    From the same network, used to understand the first basic concepts about Bayesian networks, we assume the same topological order, such as: \vspace{3.5pt}
    \begin{center}
        $[Cloudy, Sprinkler, Rain, WetGrass]$
    \end{center} \vspace{3.5pt}

    By the sampling algorithm, we have to associate a sample value with each node:
    \begin{enumerate}
        \item $\mathbf{P}(Cloudy)$. \\
        From the initial CPT, the boolean variable has the same probability to assume $True$ or $False$, $\langle 0.5, 0.5\rangle$. In this case, the algorithm gives us 0.1,
        so $Cloudy = True$.
        \item $\mathbf{P}(Sprinkler|Cloudy=True)$. \\
        Again, we know that $P(Sprinkler=True|Cloudy=True) = 0.1$ and $P(Sprinkler=False|Cloudy=True) = 0.9$. The method return us 0.2, so $Sprinkler = False$. 
        \item $\mathbf{P}(Rain|Cloudy=True)$. \\
        Analyzing the CPT, we have $P(Rain=True|Cloudy=True) = 0.8$ and $P(Rain=False|Cloudy=True) = 0.2$. The sample value is 0.7, the boolean value associated is 
        $True$.
        \item $\mathbf{P}(WetGrass|Sprinkler=False, Rain=True)$. \\
        Another time, we use the conditional probability table to see what happens if are given these evidences. Therefore, $P(WetGrass = True|Sprinkler=False, Rain=True) = 0.9$
        and $P(WetGrass=False|Sprinkler=False, Rain=True) = 0.1$, the final value is $True$. 
    \end{enumerate} 
    In this case, \textit{Prior-Sample} return us $\langle t,f,t,t \rangle$
\end{example}
From the previous example, we can compute which is the probability that \textit{Prior-Sample} generates a particular event. First, let $S_{PS}(x_1, \dots, x_n)$ be the probability
that a specific event is generated by the algorithm, then we have:
\begin{center}
    $S_{PS}(x_1, \dots, x_n) = \prod_{i=1}^{n}P(x_i|Parents(X_i))$
\end{center}
so, the probability that a specific event is generated is equal to the product of each conditional probability given the parents of $X_i$. Accordingly to the definition of
\textit{global semantics}, we have:
\begin{center}
    $S_{PS}(x_1, \dots, x_n) = \prod_{i=1}^{n}P(x_i|Parents(X_i)) = P(x_1 \dots x_n)$.
\end{center}
This last observation tell us exactly that the algorithm generates samples from the prior joint distribution! \vspace{3.5pt}

Generally, the answers are computed by counting the actual samples generated. Suppose there are $N$ total samples, and let $N_{PS}(x_1, \dots, x_n)$ be the number of times
the specific event occurs in the set of samples. Then we have:
\begin{center}
    $\lim_{N \rightarrow \infty} \frac{N_{PS}(x_1, \dots, x_n)}{N} = S_{PS}(x_1, \dots, x_n) = P(x_1 \dots x_n)$.
\end{center}
We expect $\frac{N_{PS}}{N}$ to converge to its expected value according to the sampling probability. \vspace{3.5pt}

For instance, consider the previously event: \textit{[t,f,t,t]}. The sampling probability for this event is:
\begin{center}
    $S_{PS}(t,f,t,t) = 0.5 \times 0.9 \times 0.8 \times 0.9 = 0.324$.
\end{center} \vspace{3.5pt}

Hence, we expect $32.4\%$ of the samples to be of this event. That is, estimates derived from \textit{Prior-Sample} are \textbf{consistent}, briefly:
\begin{center}
    $P(x_1 \dots x_n) \approx \frac{N_{PS}(x_1, \dots, x_m)}{N}$
\end{center}